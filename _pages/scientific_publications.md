---
layout: archive
title: "Scientific Publications"
permalink: /scientific_publications/
author_profile: false
---

Known Problems:

- Opaque Evaluation: Current evaluations (i.e., journal/conference reviews) are not published, and the people evaluating is not accountable for the quality of their reviews. This may lead to subjective reviews, which in turn result in unfair publication decisions. At the same time, readers of published papers are unaware of the issues the evaluators found on those, and have a harder time at identifying the limitations of published research.

- Unsuited Evaluation: Current evaluation (i.e., journal/conference reviews) is made by two to four volunteers who may not be experts on the topic. These reviewers are forced to decide on the publication of a scientific contribution without being fully aware of the state-of-the-art or its overall context.

- Unrewarded Evaluation: While having one of the more complicated and relevant roles in the scientific publication cycle, good reviewers (e.g., thorough, detailed, well-argued) are not rewarded. Additionally, posterior readers will not be able to benefit from their work due to opaque evaluation. Nonetheless, the first step in any scientific career is, or should be, reading and criticisizing lots of papers to properly understand the current state-of-the-art. 

- Lack of Replicability: Replicability is not a must for most publications, and papers which try to replicate other's work are not favored for publication. 

- Urge for Publication: Scientists are forced to publish several papers per year, regardless of their actual relevance. Research is forced to produce results in the short term, and abandon the topic for a new one once the paper is accepted. This also results in limited scientific impact on society.

- Scientific Paper Visibility is Unscientific: Papers visibility is directly affected by the impact factor of the journal or conference where it is published. Popular papers appear higher on search engines which results in a rich get richer process that impoverishes science.

- Arbitrary Journal/Conference Relevance: Journal/Conference impact factors suposedly determine their relevance, and consequently the visibility of published papers. However, impact factors are not a well defined score, and are linked to a given journal/conference, not to the papers themselves. As such, it is useless for evaluating paper quality. At the same time, impact factors are used by institutions to evaluate the contributions of researchers. This leads researchers to pursue publications of those venues instead of producing quality research.



Solutions

- Transparent Evaluation: Evaluation of papers should be an open and continuous process. Readers should be aware of the criticisms, and the level of reliability of the critics.

perpetual evaluation.

Authenticated but not signed evaluations. Limited.
Evaluations on the evaluations.

- Paper rating: include reviewer reliability and experience. User defined formulas. written reviews, numerical ratings, flags, citations

- Upon upload, proposal of reviewers by authors, avoiding conflicts of interest. Lack of reviews implies limited impact. Impact also based on previous research flags as unscientific.


## References

[Kriegeskorte, Nikolaus, Alexander Walther, and Diana Deca. "An emerging consensus for open evaluation: 18 visions for the future of scientific publishing." Frontiers in computational neuroscience 6 (2012).](http://journal.frontiersin.org/article/10.3389/fncom.2012.00094/full)
